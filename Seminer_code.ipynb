{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7BgLK9_nJC1",
        "outputId": "e8d16a99-11da-4898-9209-f98daf1f891c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ouWS5E78qB7"
      },
      "outputs": [],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMqORc-_8sp8"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVVETQax5v7W"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-WymEag8lUD"
      },
      "outputs": [],
      "source": [
        "!pip install spacy==3.7.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "Rg6OJTM41FzJ",
        "outputId": "4c8d7f31-f678-4b1a-e724-5f457ffa044a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing collected packages: networkx, node2vec\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2.1\n",
            "    Uninstalling networkx-3.2.1:\n",
            "      Successfully uninstalled networkx-3.2.1\n",
            "Successfully installed networkx-2.8.8 node2vec-0.4.6\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "af70b097bd81467786b996cc0e594913",
              "pip_warning": {
                "packages": [
                  "networkx"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install node2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdrKb_Qh8mhk"
      },
      "source": [
        "restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Xldy1oq6nZlv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "import contractions\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from textblob import TextBlob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import Dense, Bidirectional, LSTM\n",
        "import joblib\n",
        "from spacy.lang.en import stop_words\n",
        "stop_words = list(stop_words.STOP_WORDS)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import spacy_transformers\n",
        "bert_sent = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "#from nltk.parse.corenlp import CoreNLPDependencyParser\n",
        "#dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
        "from node2vec import Node2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsuq2evbnIhW"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.mkdir('/content/drive/MyDrive/DATA')\n",
        "except:\n",
        "  res = 0\n",
        "!tar -xf \"fns2020_dataset.tar.gz\" -C \"/content/drive/MyDrive/DATA\"\n",
        "!tar -xf \"/content/drive/MyDrive/DATA/training.tar.gz\" -C \"/content/drive/MyDrive/DATA/\"\n",
        "!tar -xf \"/content/drive/MyDrive/DATA/validation.tar.gz\" -C \"/content/drive/MyDrive/DATA/\"\n",
        "os.remove(\"/content/drive/MyDrive/DATA/training.tar.gz\")\n",
        "os.remove(\"/content/drive/MyDrive/DATA/validation.tar.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0eqnmaFj_td"
      },
      "outputs": [],
      "source": [
        "def ten_percent(path):\n",
        "  folders = ['training/annual_reports', 'validation/annual_reports']\n",
        "  for folder in folders:\n",
        "    path_to_folder1 = os.path.join(path, folder)\n",
        "    content_folder = os.listdir(path_to_folder1)\n",
        "    k = 0\n",
        "    for ar_file_name in content_folder:\n",
        "      if ar_file_name.split('.')[-1] == 'csv':\n",
        "        continue\n",
        "      print(\"remaining : \"+str(len(content_folder)-k))\n",
        "      k += 1\n",
        "      path_to_file1 = os.path.join(path_to_folder1, ar_file_name)\n",
        "      text_ar = open(path_to_file1)\n",
        "      text = text_ar.read()\n",
        "      try:\n",
        "        doc = nlp(text)\n",
        "      except:\n",
        "        res = 0\n",
        "\n",
        "      sentences = list(doc.sents)\n",
        "      n = int(len(sentences)/10)\n",
        "      cleaned_sentences = []\n",
        "      for sent in sentences[:n]:\n",
        "        sent = str(sent)\n",
        "        cleaned_sentences.append(sent)\n",
        "      data = {\"sentences\": cleaned_sentences}\n",
        "      df = pd.DataFrame(data)\n",
        "      name = ar_file_name[:-3]+'csv'\n",
        "      path_to_file2 = os.path.join(path_to_folder1, name)\n",
        "      try:\n",
        "        df.to_csv(path_to_file2)\n",
        "        os.remove(path_to_file1)\n",
        "      except:\n",
        "        res = 0\n",
        "        os.remove(path_to_file1)\n",
        "  return 0\n",
        "\n",
        "ten_percent('/content/drive/MyDrive/DATA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_1wxclLb05S"
      },
      "outputs": [],
      "source": [
        "def remove_telephone_numbers(sentence):\n",
        "  i = 0\n",
        "  remove = []\n",
        "  sentence = str(sentence)\n",
        "  while i < len(sentence):\n",
        "    if sentence[i] == '+':\n",
        "      count = 0\n",
        "      j = i+1\n",
        "      while j < len(sentence):\n",
        "        if sentence[j] >='0' and sentence[j] <='9':\n",
        "          count += 1\n",
        "          j += 1\n",
        "        elif sentence[j] == ' ' or sentence[j] == '(' or sentence[j] == ')':\n",
        "          j += 1\n",
        "        else:\n",
        "          break\n",
        "      print(count)\n",
        "      if count >= 10 and count <= 13:\n",
        "        sentence = sentence.replace(sentence[i:j], \"\")\n",
        "        i = -1\n",
        "    i = i+1\n",
        "  return sentence\n",
        "\n",
        "def remove_addresses_sent(text):\n",
        "  text = str(text)\n",
        "  clean_text = re.sub(r\"\\S*www.\\S+\", \"\", text)\n",
        "  clean_text = re.sub(r\"\\S*WWW.\\S+\", \"\", clean_text)\n",
        "  clean_text = re.sub(r\"\\S*.com\\S+\", \"\", clean_text)\n",
        "  clean_text = re.sub(r\"\\S*.co\\S+\", \"\", clean_text)\n",
        "  clean_text = re.sub(r\"\\S*.COM\\S+\", \"\", clean_text)\n",
        "  clean_text = re.sub(r\"\\S*.gov\\S+\", \"\", clean_text)\n",
        "  clean_text = re.sub(r\"\\S*.biz\\S+\", \"\", clean_text)\n",
        "  clean_text = re.sub(r\"\\S*.org\\S+\", \"\", clean_text)\n",
        "  clean_text = re.sub(r\"\\S*.uk\\S+\", \"\", clean_text)\n",
        "  return clean_text\n",
        "\n",
        "def preprocess_text(sentence):\n",
        "    # Remove punctuation characters\n",
        "    sentence = str(sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z0-9]\", \" \", sentence)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = nlp(sentence)\n",
        "\n",
        "    # Remove stopwords\n",
        "    lemmatized = []\n",
        "    for token in tokens:\n",
        "      token_lemma= str(token.lemma_)\n",
        "      token_lemma = token_lemma.lower()\n",
        "      if token_lemma not in stop_words and token_lemma[0] != ' ':\n",
        "        lemmatized.append(token_lemma)\n",
        "    return lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZSC4EFncKCi"
      },
      "outputs": [],
      "source": [
        "def labelling(path):\n",
        "  folders1 = ['training/annual_reports', 'validation/annual_reports']\n",
        "  folders2 = ['training/gold_summaries', 'validation/gold_summaries']\n",
        "  for i in range(len(folders1)):\n",
        "    folder = folders1[i]\n",
        "    path1 = os.path.join(path, folder)\n",
        "    contents1 = os.listdir(path1)\n",
        "    for ar_name in contents1:\n",
        "      path2 = os.path.join(path1, ar_name)\n",
        "      df1 = pd.read_csv(path2)\n",
        "      sentences = list(df1['sentences'])\n",
        "      labels = []\n",
        "      compare_sentences = []\n",
        "      path3 = os.path.join(path, folders2[i])\n",
        "      gs_files = os.listdir(path3)\n",
        "      for x in gs_files:\n",
        "        if x.split('_')[0] == ar_name.split('.')[0]:\n",
        "          path4 = os.path.join(path3, x)\n",
        "          file_pt = open(path4)\n",
        "          text = file_pt.read()\n",
        "          doc = nlp(text)\n",
        "          for sen in list(doc.sents):\n",
        "            compare_sentences.append(str(sen))\n",
        "      compare_sentences = list(set(compare_sentences))\n",
        "      #print(compare_sentences[0])\n",
        "      cleaned_sentences = []\n",
        "      for x in sentences:\n",
        "        sent = remove_telephone_numbers(x)\n",
        "        sent = remove_addresses_sent(sent)\n",
        "        expanded_text = []\n",
        "        for word in sent.split(' '):\n",
        "          expanded_text.append(contractions.fix(word))\n",
        "        expanded_text = ' '.join(expanded_text)\n",
        "        sent_cleaned = preprocess_text(expanded_text)\n",
        "        final_form = ' '.join(sent_cleaned)\n",
        "        cleaned_sentences.append(final_form)\n",
        "        if x in compare_sentences:\n",
        "          labels.append(1)\n",
        "        else:\n",
        "          labels.append(0)\n",
        "      df1['cleaned'] = cleaned_sentences\n",
        "      df1['label'] = labels\n",
        "      df1.to_csv(path2)\n",
        "\n",
        "labelling('/content/drive/MyDrive/DATA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tuie5nKOd8Fq"
      },
      "source": [
        "Calculating tf score of n-grams(<=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZmI_FUFddcz",
        "outputId": "d5da28b6-dab2-4e1e-a615-0ea5b4319450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DATA/training/annual_reports_IDF.csv\n",
            "/content/drive/MyDrive/DATA/validation/annual_reports_IDF.csv\n"
          ]
        }
      ],
      "source": [
        "def calculate_IDF(freq_dict, doc):\n",
        "    unique_words = list(set(doc))\n",
        "    for word in unique_words:\n",
        "        if word in freq_dict:\n",
        "            freq_dict[word] += 1\n",
        "        else:\n",
        "            freq_dict[word] = 1\n",
        "    return freq_dict\n",
        "\n",
        "def wrapper_calculate_IDF(path_to_training_data, n_gram):\n",
        "    freq_dict = {}\n",
        "    ars = os.listdir(path_to_training_data)\n",
        "    files_remaining = 0\n",
        "    for ar in ars:\n",
        "        #print(len(ars)-files_remaining)\n",
        "        files_remaining += 1\n",
        "        path = os.path.join(path_to_training_data, ar)\n",
        "        sentences_df = pd.read_csv(path)\n",
        "        sentences = list(sentences_df[\"cleaned\"])\n",
        "        cleaned_sentences = [] #as list of words\n",
        "        for sent in sentences:\n",
        "            sent = str(sent)\n",
        "            sent = sent.split(' ')\n",
        "            cleaned_sentences.append(sent)\n",
        "\n",
        "        n_gram_of_sentences = []\n",
        "        for n_gram_count in range(n_gram):\n",
        "            temp = []\n",
        "            for sent in cleaned_sentences:\n",
        "                for i in range(len(sent)):\n",
        "                    j = i+n_gram_count+1\n",
        "                    if j <= len(sent):\n",
        "                        n_gram_temp = ' '.join(sent[i:j])\n",
        "                        temp.append(n_gram_temp)\n",
        "            n_gram_of_sentences.extend(temp)\n",
        "        freq_dict = calculate_IDF(freq_dict, n_gram_of_sentences)\n",
        "        #print(freq_dict)\n",
        "    return freq_dict, len(ars)\n",
        "\n",
        "def save_idf(path_to_training_data):\n",
        "    idf_dict, n = wrapper_calculate_IDF(path_to_training_data, n_gram=3)\n",
        "    token_list = list(idf_dict.keys())\n",
        "    N_list = []\n",
        "    for i in range(len(token_list)):\n",
        "        N_list.append(n)\n",
        "    idf_freq_raw = list(idf_dict.values())\n",
        "    idf_freq = np.divide(N_list, idf_freq_raw)\n",
        "    idf_freq = np.log(idf_freq)\n",
        "    data = {\"tokens\": token_list, \"idf_frequency\": idf_freq_raw, \"total doucuments\":N_list, \"idf\":idf_freq}\n",
        "    df = pd.DataFrame(data)\n",
        "    df_path = path_to_training_data+'_IDF.csv'\n",
        "    print(df_path)\n",
        "    df.to_csv(df_path)\n",
        "\n",
        "def main1(path):\n",
        "  folders = ['training/annual_reports', 'validation/annual_reports']\n",
        "  for folder in folders:\n",
        "    path2 = os.path.join(path, folder)\n",
        "    save_idf(path2)\n",
        "main1('/content/drive/MyDrive/DATA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVEW-EGpeCZB"
      },
      "source": [
        "Calculating tfidf scores of ngrams(<=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x70ubkQ9d216"
      },
      "outputs": [],
      "source": [
        "def calc_tfidf(path, n_gram, idf_dict):\n",
        "  contents = os.listdir(path)\n",
        "  remain = 0\n",
        "  for ar in contents:\n",
        "    #print(len(contents)-remain)\n",
        "    remain += 1\n",
        "    tf_dict = {}\n",
        "    path1 = os.path.join(path, ar)\n",
        "    sentences_df = pd.read_csv(path1)\n",
        "    sentences = list(sentences_df[\"cleaned\"])\n",
        "\n",
        "    n_gram_of_sentences = []\n",
        "\n",
        "    for n_gram_count in range(n_gram):\n",
        "      temp = []\n",
        "      for sent in sentences:\n",
        "        sent = str(sent)\n",
        "        sent = sent.split(' ')\n",
        "        for i in range(len(sent)):\n",
        "          j = i+n_gram_count+1\n",
        "          if j <= len(sent):\n",
        "            n_gram_temp = ' '.join(sent[i:j])\n",
        "            temp.append(n_gram_temp)\n",
        "      if n_gram_count == 0:\n",
        "        doc_length = float(len(temp))\n",
        "      n_gram_of_sentences.extend(temp)\n",
        "\n",
        "    for token in n_gram_of_sentences:\n",
        "      if token in tf_dict:\n",
        "        tf_dict[token] += 1\n",
        "      else:\n",
        "        tf_dict[token] = 1\n",
        "\n",
        "    keys = tf_dict.keys()\n",
        "    for key in keys:\n",
        "      val = tf_dict[key]\n",
        "      T_len = len(key.split(' '))\n",
        "      tf_dict[key] = val/float(doc_length-T_len+1)\n",
        "\n",
        "    tf_idf_scores = []\n",
        "    if len(sentences)==0:\n",
        "      os.remove(path1)\n",
        "      continue\n",
        "    for sent in sentences:\n",
        "      sent = str(sent)\n",
        "      sent = sent.split(' ')\n",
        "      empty_indices = 0\n",
        "      for l in range(len(sent)):\n",
        "        if sent[l] == '':\n",
        "          empty_indices += 1\n",
        "      for l in range(empty_indices):\n",
        "        sent.remove('')\n",
        "\n",
        "      temp = []\n",
        "      for n_gram_count in range(n_gram):\n",
        "        for i in range(len(sent)):\n",
        "          j = i+n_gram_count+1\n",
        "          if j <= len(sent):\n",
        "            n_gram_temp = ' '.join(sent[i:j])\n",
        "            temp.append(n_gram_temp)\n",
        "      tf_idf_score = 0\n",
        "      for x in temp:\n",
        "        try:\n",
        "          score_temp = idf_dict[x]\n",
        "        except:\n",
        "          score_temp = 0\n",
        "        score_temp *= tf_dict[x]\n",
        "        tf_idf_score += score_temp\n",
        "      tf_idf_scores.append(tf_idf_score)\n",
        "    max_val = np.max(tf_idf_scores)\n",
        "    tf_idf_scores = np.divide(tf_idf_scores, max_val)\n",
        "    #print(tf_idf_scores)\n",
        "    sentences_df[\"tf_idf_n_gram_\"+str(n_gram)] = np.array(tf_idf_scores)\n",
        "    sentences_df.to_csv(path1)\n",
        "\n",
        "\n",
        "def main2(path, n_gram = 3):\n",
        "  folders = ['training/annual_reports', 'validation/annual_reports']\n",
        "  for folder in folders:\n",
        "    path1 = os.path.join(path, folder)\n",
        "    idf_df = pd.read_csv(path1+'_IDF.csv')\n",
        "    tokens = idf_df[\"tokens\"]\n",
        "    idf_score = idf_df[\"idf\"]\n",
        "    idf_dict = {}\n",
        "    for i in range(len(tokens)):\n",
        "      idf_dict[tokens[i]] = idf_score[i]\n",
        "    for i in range(n_gram):\n",
        "      calc_tfidf(path1, i+1, idf_dict)\n",
        "main2('/content/drive/MyDrive/DATA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRXFWy0UHqoz"
      },
      "source": [
        "Sentence Bert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DA2qZYTy5GYl"
      },
      "outputs": [],
      "source": [
        "def bert_embedding_generate(path, path_bert):\n",
        "  contents = os.listdir(path)\n",
        "  l = 0\n",
        "  for ar in contents:\n",
        "    print(\"remaining : \"+str(len(contents)- l))\n",
        "    l += 1\n",
        "    path1 = os.path.join(path, ar)\n",
        "    df = pd.read_csv(path1)\n",
        "    sentences = list(df['sentences'])\n",
        "\n",
        "    embedding = []\n",
        "    for sent in sentences:\n",
        "      doc = bert_sent(sent)\n",
        "      tokvecs = doc._.trf_data.last_hidden_layer_state[0]\n",
        "      embed = np.mean(tokvecs.data, axis=0)\n",
        "      embed = list(embed)\n",
        "\n",
        "      sentiment_score = TextBlob(sent)\n",
        "      embed.extend([sentiment_score.sentiment[0], sentiment_score.sentiment[1]])\n",
        "      embedding.append(embed)\n",
        "\n",
        "    name = ar[:-4]+'.npy'\n",
        "    path_to_npy_file = os.path.join(path_bert, name)\n",
        "    with open(path_to_npy_file, 'wb') as f:\n",
        "      np.save(f, np.array(embedding))\n",
        "\n",
        "def main3(path):\n",
        "  folders = ['training/annual_reports', 'validation/annual_reports']\n",
        "  for folder in folders:\n",
        "    path1 = os.path.join(path, folder)\n",
        "    path2 = os.path.join(path, folder+'_bert_embedding')\n",
        "    try:\n",
        "      os.mkdir(path2)\n",
        "      print(path2)\n",
        "    except:\n",
        "      res = 0\n",
        "\n",
        "    bert_embedding_generate(path1, path2)\n",
        "main3('/content/drive/MyDrive/DATA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tS3mJV5H3aQ"
      },
      "source": [
        "Sentence Node Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qjlpzp9l1ex4"
      },
      "outputs": [],
      "source": [
        "def node_embedding_generate(path, path_bert):\n",
        "  contents = os.listdir(path)\n",
        "  l = 0\n",
        "  \"\"\"contents2 = os.listdir(path_bert)\n",
        "  for x in contents2:\n",
        "    contents.remove(x[:-3]+'csv')\"\"\"\n",
        "  for ar in contents:\n",
        "    print(\"remaining : \"+str(len(contents)- l))\n",
        "    l += 1\n",
        "    path1 = os.path.join(path, ar)\n",
        "    df = pd.read_csv(path1)\n",
        "    sentences = list(df['sentences'])\n",
        "\n",
        "    embedding = []\n",
        "    for sent in sentences:\n",
        "      sent = str(sent)\n",
        "      doc = nlp(sent)\n",
        "      G = nx.DiGraph()\n",
        "      for token in doc:\n",
        "        G.add_node(token.i, label=token.text)\n",
        "      for token in doc:\n",
        "        if token.dep_ == \"ROOT\":\n",
        "          root_token = token\n",
        "          idx = token.i\n",
        "        G.add_edge(token.head.i, token.i, label=token.dep_)\n",
        "      G = G.reverse()\n",
        "      node2vec = Node2Vec(G, dimensions=128, walk_length=80, num_walks=10)\n",
        "      model = node2vec.fit(window=10, min_count=1)\n",
        "      required_embedding = model.wv.get_vector(idx)\n",
        "      word_vector = nlp(root_token.text).vector\n",
        "      res = list(np.concatenate((required_embedding, word_vector), axis=0))\n",
        "      embedding.append(res)\n",
        "    name = ar[:-4]+'.npy'\n",
        "    path_to_npy_file = os.path.join(path_bert, name)\n",
        "    with open(path_to_npy_file, 'wb') as f:\n",
        "      np.save(f, np.array(embedding))\n",
        "\n",
        "def main3(path):\n",
        "  folders = ['training/annual_reports', 'validation/annual_reports']\n",
        "  for folder in folders:\n",
        "    path1 = os.path.join(path, folder)\n",
        "    path2 = os.path.join(path, folder+'_node_embedding')\n",
        "    try:\n",
        "      os.mkdir(path2)\n",
        "    except:\n",
        "      res = 0\n",
        "\n",
        "    node_embedding_generate(path1, path2)\n",
        "main3('/content/drive/MyDrive/DATA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHWp0IyXXUSp"
      },
      "source": [
        "Training model1 using bert embeddigs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW4I441ZENhb"
      },
      "outputs": [],
      "source": [
        "def train_bert_model1(path1, path2):\n",
        "  contents = os.listdir(path1)\n",
        "  contents_bert = os.listdir(path2)\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  for i in range(len(contents)):\n",
        "    print(i)\n",
        "    x = contents[i]\n",
        "    y = x[:-3]+'npy'\n",
        "    path3 = os.path.join(path1, x)\n",
        "    path4 = os.path.join(path2, y)\n",
        "\n",
        "    df = pd.read_csv(path3)\n",
        "    temp2 = list(df['label'])\n",
        "    temp1 = list(np.load(path4))\n",
        "    if len(temp1) == len(temp2):\n",
        "      x_train.extend(temp1)\n",
        "      y_train.extend(temp2)\n",
        "    else:\n",
        "      print(\"diff length : \"+x)\n",
        "  x_train = np.array(x_train)\n",
        "  y_train = np.array(y_train)\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(x_train)\n",
        "  x_train_scaled = scaler.transform(x_train)\n",
        "  joblib.dump(scaler, '/content/drive/MyDrive/DATA/bert_embedding_scaler.gz')\n",
        "\n",
        "  X_train = []\n",
        "  for x in x_train_scaled:\n",
        "    X_train.append(x.reshape(1, len(x)))\n",
        "  X_train = np.array(X_train)\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(1, 770))\n",
        "  l1 = Bidirectional(LSTM(units=50))(inputs)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(l1)\n",
        "  model_1 = tf.keras.Model(inputs=inputs, outputs =outputs)\n",
        "\n",
        "  with open('/content/drive/MyDrive/DATA/lstm_model_bert_embedding_summary.txt', 'w') as f:\n",
        "    model_1.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "  model_1.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  checkpoint_filepath = '/content/drive/MyDrive/DATA/lstm_model_bert_embedding_weights.h5'\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "  history = model_1.fit(X_train, X_train, epochs=100, batch_size=128, validation_split=0.3, callbacks=[model_checkpoint_callback])\n",
        "  model_1.load_weights(checkpoint_filepath)\n",
        "  model_1.save('/content/drive/MyDrive/DATA/bert_embedding_lstm.keras')\n",
        "\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.savefig('/content/drive/MyDrive/DATA/bert_embedding_lstm_accuracy.png')\n",
        "\n",
        "\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.savefig('/content/drive/MyDrive/DATA/bert_embedding_lstm_loss.png')\n",
        "\n",
        "def main(path):\n",
        "  folders = ['training/annual_reports', 'training/annual_reports_bert_embedding']\n",
        "  path1 = os.path.join(path, folders[0])\n",
        "  path2 = os.path.join(path, folders[1])\n",
        "\n",
        "  train_bert_model1(path1, path2)\n",
        "main('/content/drive/MyDrive/DATA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOy1DnGUXcGZ"
      },
      "source": [
        "Training model2 using node embeddigs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5vXoPjUOLZh"
      },
      "outputs": [],
      "source": [
        "def train_node_model2(path1, path2):\n",
        "  contents = os.listdir(path1)\n",
        "  contents_bert = os.listdir(path2)\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  for i in range(len(contents)):\n",
        "    print(i)\n",
        "    x = contents[i]\n",
        "    y = x[:-3]+'npy'\n",
        "    path3 = os.path.join(path1, x)\n",
        "    path4 = os.path.join(path2, y)\n",
        "\n",
        "    df = pd.read_csv(path3)\n",
        "    temp2 = list(df['label'])\n",
        "    temp1 = list(np.load(path4))\n",
        "    if len(temp1) == len(temp2):\n",
        "      x_train.extend(temp1)\n",
        "      y_train.extend(temp2)\n",
        "    else:\n",
        "      print(\"diff length : \"+x)\n",
        "  x_train = np.array(x_train)\n",
        "  y_train = np.array(y_train)\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(x_train)\n",
        "  x_train_scaled = scaler.transform(x_train)\n",
        "  joblib.dump(scaler, '/content/drive/MyDrive/DATA/node_embedding_scaler.gz')\n",
        "\n",
        "  X_train = []\n",
        "  for x in x_train_scaled:\n",
        "    X_train.append(x.reshape(1, len(x)))\n",
        "  X_train = np.array(X_train)\n",
        "  inputs = tf.keras.Input(shape=(1, 224))\n",
        "  l1 = Bidirectional(LSTM(units=50))(inputs)\n",
        "  outputs = tf.keras.layers.Dense(1, activation='sigmoid')(l1)\n",
        "  model_1 = tf.keras.Model(inputs=inputs, outputs =outputs)\n",
        "\n",
        "  with open('/content/drive/MyDrive/DATA/lstm_model_node_embedding_summary.txt', 'w') as f:\n",
        "    model_1.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "  model_1.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  checkpoint_filepath = '/content/drive/MyDrive/DATA/lstm_model_node_embedding_weights.h5'\n",
        "  model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "  history = model_1.fit(X_train, X_train, epochs=100, batch_size=128, validation_split=0.3, callbacks=[model_checkpoint_callback])\n",
        "  model_1.load_weights(checkpoint_filepath)\n",
        "  model_1.save('/content/drive/MyDrive/DATA/node_embedding_lstm.keras')\n",
        "\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.savefig('/content/drive/MyDrive/DATA/node_embedding_lstm_accuracy.png')\n",
        "\n",
        "\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.savefig('/content/drive/MyDrive/DATA/node_embedding_lstm_loss.png')\n",
        "\n",
        "def main(path):\n",
        "  folders = ['training/annual_reports', 'training/annual_reports_node_embedding']\n",
        "  path1 = os.path.join(path, folders[0])\n",
        "  path2 = os.path.join(path, folders[1])\n",
        "\n",
        "  train_node_model2(path1, path2)\n",
        "main('/content/drive/MyDrive/DATA')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label prediction model1"
      ],
      "metadata": {
        "id": "2PbIazFPZWWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = joblib.load('/content/drive/MyDrive/DATA/bert_embedding_scaler.gz')\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/DATA/bert_embedding_lstm.keras')\n",
        "path = \"/content/drive/MyDrive/DATA/validation/annual_reports\"\n",
        "path_bert = \"/content/drive/MyDrive/DATA/validation/annual_reports_bert_embedding\"\n",
        "\n",
        "def predict_labels(x_test, y_test, scalar):\n",
        "  X_test = []\n",
        "\n",
        "  x_test = np.array(x_test)\n",
        "  y_test = np.array(y_test)\n",
        "  x_test_scaled = scaler.transform(x_test)\n",
        "  for x in x_test_scaled:\n",
        "    X_test.append(x.reshape(1, len(x)))\n",
        "  X_test = np.array(X_test)\n",
        "  return X_test, y_test\n",
        "\n",
        "\n",
        "#saving the predictd labels\n",
        "contents = os.listdir(path)\n",
        "for i in range(len(contents)):\n",
        "  print(i)\n",
        "  x = contents[i]\n",
        "  y = x[:-3]+'npy'\n",
        "  path1 = os.path.join(path, x)\n",
        "  path2 = os.path.join(path_bert, y)\n",
        "\n",
        "  df = pd.read_csv(path1)\n",
        "  temp2 = list(df['label'])\n",
        "  temp1 = np.load(path2)\n",
        "  if len(temp1) == len(temp2):\n",
        "    X_test, y_test = predict_labels(temp1, temp2, scaler)\n",
        "    pred = model.predict(X_test)\n",
        "    y_pred = []\n",
        "    for x in pred:\n",
        "      if x > .3:\n",
        "        y_pred.append(1)\n",
        "      else:\n",
        "        y_pred.append(0)\n",
        "    print(path1)\n",
        "    df['predicted label'] = y_pred\n",
        "    df.to_csv(path1)\n",
        "  else:\n",
        "    print(\"diff length : \"+x)"
      ],
      "metadata": {
        "id": "wIczePuEuUGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Candidate sequences model1"
      ],
      "metadata": {
        "id": "MQGwkkNDZk1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import joblib\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/DATA/bert_embedding_lstm.keras')\n",
        "scaler_node = joblib.load('/content/drive/MyDrive/DATA/node_embedding_scaler.gz')\n",
        "scaler_bert = joblib.load('/content/drive/MyDrive/DATA/bert_embedding_scaler.gz')\n",
        "\n",
        "def predict_labels(x_test, y_test, scaler):\n",
        "  X_test = []\n",
        "\n",
        "  X_test = np.array(X_test)\n",
        "  x_test = np.array(x_test)\n",
        "  y_test = np.array(y_test)\n",
        "  x_test_scaled = scaler.transform(x_test)\n",
        "  for x in x_test_scaled:\n",
        "    X_test.append(x.reshape(1, len(x)))\n",
        "  return X_test, y_test\n",
        "\n",
        "def get_chunks(sent_list):\n",
        "  sent_length = []\n",
        "  for sent in sent_list:\n",
        "    sent = str(sent)\n",
        "    temp = sent.split(' ')\n",
        "    sent_length.append(len(temp))\n",
        "  Seq_indices = []\n",
        "\n",
        "  start = 0\n",
        "  step = 0\n",
        "  count = 0\n",
        "  while count < len(sent_length):\n",
        "    if step < 1000:\n",
        "      step += sent_length[count]\n",
        "      count += 1\n",
        "    else:\n",
        "      stop = count - 1\n",
        "      Seq_indices.append((start, stop))\n",
        "      start = count\n",
        "      step = sent_length[count]\n",
        "      count += 1\n",
        "\n",
        "  Seq_indices.append((start, count-1))\n",
        "  return Seq_indices\n",
        "\n",
        "def get_text(sent_list, tup):\n",
        "  a, b = tup[0], tup[1]\n",
        "  i = a\n",
        "  text = ''\n",
        "  while i <= b:\n",
        "    text += str(sent_list[i])\n",
        "    if i < b:\n",
        "      text += ' '\n",
        "    i += 1\n",
        "  return text\n",
        "\n",
        "def get_node_embed_score(X_test, tup):\n",
        "  a, b = tup[0], tup[1]\n",
        "  i = a\n",
        "  score_list = []\n",
        "  while i <= b:\n",
        "    temp = X_test[i]\n",
        "    score_list.append(temp)\n",
        "    i += 1\n",
        "  deno = b-a+1\n",
        "  res = np.sum(score_list)/float(deno)\n",
        "  return res\n",
        "\n",
        "def normalize(X_text):\n",
        "  mean_list = []\n",
        "  for x in X_text:\n",
        "    mean_list.append(np.mean(x))\n",
        "  max_val = np.max(mean_list)\n",
        "  min_val = np.min(mean_list)\n",
        "  deno = max_val - min_val\n",
        "  mean_list = np.array(mean_list)\n",
        "  mean_list = mean_list - min_val\n",
        "  min_val /= float(deno)\n",
        "  return mean_list\n",
        "\n",
        "node_path = \"/content/drive/MyDrive/DATA/validation/annual_reports_node_embedding\"\n",
        "bert_path = \"/content/drive/MyDrive/DATA/validation/annual_reports_bert_embedding\"\n",
        "sent_path = \"/content/drive/MyDrive/DATA/validation/annual_reports\"\n",
        "save_summary_path = \"/content/drive/MyDrive/DATA/SUM_Bert\"\n",
        "try:\n",
        "  os.mkdir(save_summary_path)\n",
        "except:\n",
        "  print(\"exists!\")\n",
        "#sum_path = \"/home/student/Music/fns2020_dataset/node_embedding/vaild_sents\"\n",
        "contents = os.listdir(sent_path)\n",
        "contents_bert = os.listdir(bert_path)\n",
        "contents_node = os.listdir(node_path)\n",
        "\n",
        "x_test = []\n",
        "pred_dict = {}\n",
        "\n",
        "\n",
        "for i in range(len(contents)):\n",
        "  print(i)\n",
        "  x = contents[i]\n",
        "  y = x[:-3]+'npy'\n",
        "  path1 = os.path.join(sent_path, x)\n",
        "  path2 = os.path.join(bert_path, y)\n",
        "  path3 = os.path.join(node_path, y)\n",
        "  if x in contents and y in contents_bert and y in contents_node:\n",
        "    df = pd.read_csv(path1)\n",
        "    sents = list(df['sentences'])\n",
        "    labels = list(df['predicted label'])\n",
        "    bert_feat = np.load(path2)\n",
        "    node_feat = np.load(path3)\n",
        "    bert_score = normalize(bert_feat)\n",
        "    node_score = normalize(node_feat)\n",
        "    tfidf_1 = list(df['tf_idf_n_gram_1'])\n",
        "    tfidf_2 = list(df['tf_idf_n_gram_2'])\n",
        "    tfidf_3 = list(df['tf_idf_n_gram_3'])\n",
        "    selected_sents = []\n",
        "    seq_text = []\n",
        "    seq_bert_score = []\n",
        "    seq_node_score = []\n",
        "    tfidf_1_score = []\n",
        "    tfidf_2_score = []\n",
        "    tfidf_3_score = []\n",
        "    for i in range(len(sents)):\n",
        "      if labels[i] == 1:\n",
        "        selected_sents.append(sents[i])\n",
        "    if len(selected_sents) > 0:\n",
        "      seq_ids = get_chunks(selected_sents)\n",
        "      print(seq_ids)\n",
        "      for tup in seq_ids:\n",
        "        seq_text.append(get_text(selected_sents, tup))\n",
        "        seq_bert_score.append(get_node_embed_score(bert_score, tup))\n",
        "        seq_node_score.append(get_node_embed_score(node_score, tup))\n",
        "        tfidf_1_score.append(get_node_embed_score(tfidf_1, tup))\n",
        "        tfidf_2_score.append(get_node_embed_score(tfidf_2, tup))\n",
        "        tfidf_3_score.append(get_node_embed_score(tfidf_3, tup))\n",
        "      data = {\"summary\": seq_text, \"bert_score\": seq_bert_score, \"node_score\": seq_node_score, \"tf_idf_n_gram_1_score\": tfidf_1_score, \"tf_idf_n_gram_2_score\": tfidf_2_score, \"tf_idf_n_gram_3_score\": tfidf_3_score}\n",
        "      df_new = pd.DataFrame(data)\n",
        "      fin_path = os.path.join(save_summary_path, x)\n",
        "      df_new.to_csv(fin_path)"
      ],
      "metadata": {
        "id": "KkZFNhostpRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label prediction model2"
      ],
      "metadata": {
        "id": "LJ2I9kcRZijG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = joblib.load('/content/drive/MyDrive/DATA/node_embedding_scaler.gz')\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/DATA/node_embedding_lstm.keras')\n",
        "path = \"/content/drive/MyDrive/DATA/validation/annual_reports\"\n",
        "path_bert = \"/content/drive/MyDrive/DATA/validation/annual_reports_node_embedding\"\n",
        "\n",
        "def predict_labels(x_test, y_test, scalar):\n",
        "  X_test = []\n",
        "\n",
        "  x_test = np.array(x_test)\n",
        "  y_test = np.array(y_test)\n",
        "  x_test_scaled = scaler.transform(x_test)\n",
        "  for x in x_test_scaled:\n",
        "    X_test.append(x.reshape(1, len(x)))\n",
        "  X_test = np.array(X_test)\n",
        "  return X_test, y_test\n",
        "\n",
        "\n",
        "#saving the predictd labels\n",
        "contents = os.listdir(path)\n",
        "for i in range(len(contents)):\n",
        "  print(i)\n",
        "  x = contents[i]\n",
        "  y = x[:-3]+'npy'\n",
        "  path1 = os.path.join(path, x)\n",
        "  path2 = os.path.join(path_bert, y)\n",
        "\n",
        "  df = pd.read_csv(path1)\n",
        "  temp2 = list(df['label'])\n",
        "  temp1 = np.load(path2)\n",
        "  if len(temp1) == len(temp2):\n",
        "    X_test, y_test = predict_labels(temp1, temp2, scaler)\n",
        "    pred = model.predict(X_test)\n",
        "    y_pred = []\n",
        "    for x in pred:\n",
        "      if x > .3:\n",
        "        y_pred.append(1)\n",
        "      else:\n",
        "        y_pred.append(0)\n",
        "    print(path1)\n",
        "    df['predicted label'] = y_pred\n",
        "    df.to_csv(path1)\n",
        "  else:\n",
        "    print(\"diff length : \"+x)"
      ],
      "metadata": {
        "id": "RRIwcGbVuxc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Candidate sequences model2"
      ],
      "metadata": {
        "id": "Ou8xCD-QZqSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import joblib\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/DATA/node_embedding_lstm.keras')\n",
        "scaler_node = joblib.load('/content/drive/MyDrive/DATA/node_embedding_scaler.gz')\n",
        "scaler_bert = joblib.load('/content/drive/MyDrive/DATA/bert_embedding_scaler.gz')\n",
        "\n",
        "def predict_labels(x_test, y_test, scaler):\n",
        "  X_test = []\n",
        "\n",
        "  X_test = np.array(X_test)\n",
        "  x_test = np.array(x_test)\n",
        "  y_test = np.array(y_test)\n",
        "  x_test_scaled = scaler.transform(x_test)\n",
        "  for x in x_test_scaled:\n",
        "    X_test.append(x.reshape(1, len(x)))\n",
        "  return X_test, y_test\n",
        "\n",
        "def get_chunks(sent_list):\n",
        "  sent_length = []\n",
        "  for sent in sent_list:\n",
        "    sent = str(sent)\n",
        "    temp = sent.split(' ')\n",
        "    sent_length.append(len(temp))\n",
        "  Seq_indices = []\n",
        "\n",
        "  start = 0\n",
        "  step = 0\n",
        "  count = 0\n",
        "  while count < len(sent_length):\n",
        "    if step < 1000:\n",
        "      step += sent_length[count]\n",
        "      count += 1\n",
        "    else:\n",
        "      stop = count - 1\n",
        "      Seq_indices.append((start, stop))\n",
        "      start = count\n",
        "      step = sent_length[count]\n",
        "      count += 1\n",
        "\n",
        "  Seq_indices.append((start, count-1))\n",
        "  return Seq_indices\n",
        "\n",
        "def get_text(sent_list, tup):\n",
        "  a, b = tup[0], tup[1]\n",
        "  i = a\n",
        "  text = ''\n",
        "  while i <= b:\n",
        "    text += str(sent_list[i])\n",
        "    if i < b:\n",
        "      text += ' '\n",
        "    i += 1\n",
        "  return text\n",
        "\n",
        "def get_node_embed_score(X_test, tup):\n",
        "  a, b = tup[0], tup[1]\n",
        "  i = a\n",
        "  score_list = []\n",
        "  while i <= b:\n",
        "    temp = X_test[i]\n",
        "    score_list.append(temp)\n",
        "    i += 1\n",
        "  deno = b-a+1\n",
        "  res = np.sum(score_list)/float(deno)\n",
        "  return res\n",
        "\n",
        "def normalize(X_text):\n",
        "  mean_list = []\n",
        "  for x in X_text:\n",
        "    mean_list.append(np.mean(x))\n",
        "  max_val = np.max(mean_list)\n",
        "  min_val = np.min(mean_list)\n",
        "  deno = max_val - min_val\n",
        "  mean_list = np.array(mean_list)\n",
        "  mean_list = mean_list - min_val\n",
        "  min_val /= float(deno)\n",
        "  return mean_list\n",
        "\n",
        "node_path = \"/content/drive/MyDrive/DATA/validation/annual_reports_node_embedding\"\n",
        "bert_path = \"/content/drive/MyDrive/DATA/validation/annual_reports_bert_embedding\"\n",
        "sent_path = \"/content/drive/MyDrive/DATA/validation/annual_reports\"\n",
        "save_summary_path = \"/content/drive/MyDrive/DATA/SUM_Node\"\n",
        "try:\n",
        "  os.mkdir(save_summary_path)\n",
        "except:\n",
        "  print(\"exists!\")\n",
        "#sum_path = \"/home/student/Music/fns2020_dataset/node_embedding/vaild_sents\"\n",
        "contents = os.listdir(sent_path)\n",
        "contents_bert = os.listdir(bert_path)\n",
        "contents_node = os.listdir(node_path)\n",
        "\n",
        "x_test = []\n",
        "pred_dict = {}\n",
        "\n",
        "\n",
        "for i in range(len(contents)):\n",
        "  print(i)\n",
        "  x = contents[i]\n",
        "  y = x[:-3]+'npy'\n",
        "  path1 = os.path.join(sent_path, x)\n",
        "  path2 = os.path.join(bert_path, y)\n",
        "  path3 = os.path.join(node_path, y)\n",
        "  if x in contents and y in contents_bert and y in contents_node:\n",
        "    df = pd.read_csv(path1)\n",
        "    sents = list(df['sentences'])\n",
        "    labels = list(df['predicted label'])\n",
        "    bert_feat = np.load(path2)\n",
        "    node_feat = np.load(path3)\n",
        "    bert_score = normalize(bert_feat)\n",
        "    node_score = normalize(node_feat)\n",
        "    tfidf_1 = list(df['tf_idf_n_gram_1'])\n",
        "    tfidf_2 = list(df['tf_idf_n_gram_2'])\n",
        "    tfidf_3 = list(df['tf_idf_n_gram_3'])\n",
        "    selected_sents = []\n",
        "    seq_text = []\n",
        "    seq_bert_score = []\n",
        "    seq_node_score = []\n",
        "    tfidf_1_score = []\n",
        "    tfidf_2_score = []\n",
        "    tfidf_3_score = []\n",
        "    for i in range(len(sents)):\n",
        "      if labels[i] == 1:\n",
        "        selected_sents.append(sents[i])\n",
        "    if len(selected_sents) > 0:\n",
        "      seq_ids = get_chunks(selected_sents)\n",
        "      print(seq_ids)\n",
        "      for tup in seq_ids:\n",
        "        seq_text.append(get_text(selected_sents, tup))\n",
        "        seq_bert_score.append(get_node_embed_score(bert_score, tup))\n",
        "        seq_node_score.append(get_node_embed_score(node_score, tup))\n",
        "        tfidf_1_score.append(get_node_embed_score(tfidf_1, tup))\n",
        "        tfidf_2_score.append(get_node_embed_score(tfidf_2, tup))\n",
        "        tfidf_3_score.append(get_node_embed_score(tfidf_3, tup))\n",
        "      data = {\"summary\": seq_text, \"bert_score\": seq_bert_score, \"node_score\": seq_node_score, \"tf_idf_n_gram_1_score\": tfidf_1_score, \"tf_idf_n_gram_2_score\": tfidf_2_score, \"tf_idf_n_gram_3_score\": tfidf_3_score}\n",
        "      df_new = pd.DataFrame(data)\n",
        "      fin_path = os.path.join(save_summary_path, x)\n",
        "      df_new.to_csv(fin_path)"
      ],
      "metadata": {
        "id": "tnA9R0JPu7b0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence score calculation"
      ],
      "metadata": {
        "id": "1jKYIu74ZuSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "params = [(0.5, 0.5, 0, 1), (0.5, 0, 0.5, 1), (0.33, 0.33, 0.33, 1), (0.5, 0.5, 0, 2), (0.5, 0, 0.5, 2), (0.33, 0.33, 0.33, 2), (0.5, 0.5, 0, 3), (0.5, 0, 0.5, 3), (0.33, 0.33, 0.33, 3),\n",
        "(1, 0, 0, 1), (1, 0, 0, 2), (1, 0, 0, 3)]\n",
        "\n",
        "summaries = ['/content/drive/MyDrive/DATA/SUM_Bert', '/content/drive/MyDrive/DATA/SUM_Node']\n",
        "project_sys = ['/content/drive/MyDrive/DATA/TIBER_1_Bert_Model', '/content/drive/MyDrive/DATA/TIBER_2_Node_Model']\n",
        "project_ref = ['system', 'reference']\n",
        "for x in project_sys:\n",
        "  try:\n",
        "    os.mkdir(x)\n",
        "    for y in project_ref:\n",
        "      os.mkdir(os.path.join(x, y))\n",
        "  except:\n",
        "    res = 0\n",
        "\n",
        "alpha = []\n",
        "beta = []\n",
        "gamma = []\n",
        "tfidf_n_gram = []\n",
        "sys = []\n",
        "for i in range(len(summaries)):\n",
        "  summ = summaries[i]\n",
        "  contents1 = os.listdir(summ)\n",
        "  for k in range(len(params)):\n",
        "    param = params[k]\n",
        "    alpha.append(param[0])\n",
        "    beta.append(param[1])\n",
        "    gamma.append(param[2])\n",
        "    tfidf_n_gram.append(param[3])\n",
        "    sys.append(k+1)\n",
        "    for j in range(len(contents1)):\n",
        "     try:\n",
        "      folder_path = os.path.join(summ, contents1[j])\n",
        "      df = pd.read_csv(folder_path)\n",
        "      sentences = list(df[\"summary\"])\n",
        "      bert_score = list(df[\"bert_score\"])\n",
        "      node_score = list(df[\"node_score\"])\n",
        "      tfidf = list(df[\"tf_idf_n_gram_\"+str(param[3])+\"_score\"])\n",
        "      scores = param[0]*np.array(tfidf) + param[1]*np.array(bert_score) + param[2]*np.array(node_score)\n",
        "      #print(param, np.max(scores), np.argmax(scores), len(scores))\n",
        "      #print(scores)\n",
        "      id_text = np.argmax(scores)\n",
        "      text = sentences[id_text]\n",
        "      text_name = contents1[j][:-4]+'_system'+str(k+1)+'.txt'\n",
        "      text_path = os.path.join(project_sys[i], 'system')\n",
        "      text_path = os.path.join(text_path, text_name)\n",
        "      Path(text_path).touch()\n",
        "      text_ptr = open(text_path, 'w')\n",
        "      text_ptr.write(text)\n",
        "      text_ptr.close()\n",
        "     except:\n",
        "      res = 0\n",
        "data = {'alpha':alpha, 'beta':beta, 'gamma':gamma, 'tfidf_n_gram':tfidf_n_gram, 'system file':sys}\n",
        "df_data = pd.DataFrame(data)\n",
        "df_data.to_csv('/content/drive/MyDrive/DATA/param_to_system_file_mapping.csv')"
      ],
      "metadata": {
        "id": "i3UHgTa4vHGz"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Renaming gold summaries as reference"
      ],
      "metadata": {
        "id": "mM4Bav4gZ1TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_val_gs = '/content/drive/MyDrive/DATA/validation/gold_summaries'\n",
        "path_vsl_ref = ['/content/drive/MyDrive/DATA/TIBER_1_Bert_Model/reference', '/content/drive/MyDrive/DATA/TIBER_2_Node_Model/reference']\n",
        "contents = os.listdir(path_val_gs)\n",
        "unique_files = []\n",
        "for x in contents:\n",
        "  unique_files.append(x.split('_')[0])\n",
        "unique_files = list(set(unique_files))\n",
        "for x in unique_files:\n",
        "  i = 1\n",
        "  for y in contents:\n",
        "    if y.split('_')[0] == x:\n",
        "      src_path = os.path.join(path_val_gs,y)\n",
        "      name = x+'_reference'+str(i)+'.txt'\n",
        "      dest_path1 = os.path.join(path_vsl_ref[0],name)\n",
        "      dest_path2 = os.path.join(path_vsl_ref[1],name)\n",
        "      shutil.copy(src_path, dest_path1)\n",
        "      shutil.copy(src_path, dest_path2)\n",
        "      i += 1"
      ],
      "metadata": {
        "id": "xGZdq8YKOZbN"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rouge score calculation"
      ],
      "metadata": {
        "id": "hhkMpwPyaQ3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kavgan/ROUGE-2.0.git\n"
      ],
      "metadata": {
        "id": "WKKTk_lVQwH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/ROUGE-2.0/versions/v1.2.2/rouge2_v1.2.2_runnable.zip'"
      ],
      "metadata": {
        "id": "oaP-c9JDXXRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TIBER1\n",
        "try:\n",
        "    shutil.rmtree('/content/v1.2.2/projects/test-summarization/reference')\n",
        "    shutil.rmtree('/content/v1.2.2/projects/test-summarization/system')\n",
        "except OSError as e:\n",
        "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "shutil.copytree('/content/drive/MyDrive/DATA/TIBER_1_Bert_Model/system','/content/v1.2.2/projects/test-summarization/system')\n",
        "shutil.copytree('/content/drive/MyDrive/DATA/TIBER_1_Bert_Model/reference','/content/v1.2.2/projects/test-summarization/reference')\n",
        "%cd '/content/v1.2.2'\n",
        "subprocess.call([\"java\",\"-jar\",\"rouge2-1.2.2.jar\"])\n",
        "shutil.copy('/content/v1.2.2/results.csv','/content/results_model1.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "rzbNCmBaRyji",
        "outputId": "d737e58e-dfbf-4fde-9f9e-4a2069e13010"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/v1.2.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/results_model1.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TIBER2\n",
        "try:\n",
        "    shutil.rmtree('/content/v1.2.2/projects/test-summarization/reference')\n",
        "    shutil.rmtree('/content/v1.2.2/projects/test-summarization/system')\n",
        "except OSError as e:\n",
        "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
        "shutil.copytree('/content/drive/MyDrive/DATA/TIBER_1_Bert_Model/system','/content/v1.2.2/projects/test-summarization/system')\n",
        "shutil.copytree('/content/drive/MyDrive/DATA/TIBER_1_Bert_Model/reference','/content/v1.2.2/projects/test-summarization/reference')\n",
        "%cd '/content/v1.2.2'\n",
        "subprocess.call([\"java\",\"-jar\",\"rouge2-1.2.2.jar\"])\n",
        "shutil.copy('/content/v1.2.2/results.csv','/content/results_model2.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CQwaLQI9YNMG",
        "outputId": "395e170e-40fe-4180-e20a-382aba2f7674"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/v1.2.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/results_model2.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}